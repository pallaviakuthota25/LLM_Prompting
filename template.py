# -*- coding: utf-8 -*-
"""template1.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/142OMxAL_dzCWi_cQr7-FanKjGk6Btf-z
"""

import transformers
import sys
import torch
import transformers
from transformers import T5Tokenizer, T5ForConditionalGeneration
import re

#Suppress warnings
transformers.logging.set_verbosity_error()
transformers.utils.logging.disable_progress_bar()
#No changes to the above code

#changes allowed from here

def llm_function(model,tokenizer,q,a,b,c,d):
    prompt = f"""Question: {q}
Options:
A. {a}
B. {b}
C. {c}
D. {d}
Answer:"""

    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=5)
    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

    # Match first character if it's A/B/C/D
    if decoded.startswith("A"):
        return "A"
    elif decoded.startswith("B"):
        return "B"
    elif decoded.startswith("C"):
        return "C"
    elif decoded.startswith("D"):
        return "D"
    else:
        # Fallback: match answer text to option
        decoded_lower = decoded.lower()
        for idx, opt in enumerate([a, b, c, d]):
            if opt.lower() in decoded_lower:
                return ["A", "B", "C", "D"][idx]

    # If nothing matches, return empty string (shouldn't happen in valid cases)
    return ""


    ##########################



#No changes allowed here
if __name__ == '__main__':

    question = sys.argv[1].strip()
    option_a = sys.argv[2].strip()
    option_b = sys.argv[3].strip()
    option_c = sys.argv[4].strip()
    option_d = sys.argv[5].strip()


    ##################### Loading Model and Tokenizer ########################
    tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-xl")
    model = T5ForConditionalGeneration.from_pretrained("google/flan-t5-xl")
    ##########################################################################

    """  Call to function that will perform the computation. """
    torch.manual_seed(42)
    out = llm_function(model,tokenizer,question,option_a,option_b,option_c,option_d)
    print(out.strip())

